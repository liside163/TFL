#!/usr/bin/env python3
"""
è½´æ‰¿æ•…éšœè¯Šæ–­å‚æ•°ä¼˜åŒ–æµç¨‹ - ç¨³å®šç‰ˆï¼ˆå¤èµ›æ•°æ®é›† - 10åˆ†ç±»ï¼‰
æ•´åˆæ•°æ®å¤„ç†ã€ç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨Optunaå¯»æ‰¾æœ€ä½³å‚æ•°ç»„åˆ
æ¯æ¬¡è¯•éªŒè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯ï¼Œå–å¹³å‡å‡†ç¡®ç‡ï¼Œæé«˜è¯„ä¼°ç¨³å®šæ€§
"""

import os
import shutil
import numpy as np
import pandas as pd
import pickle
import optuna
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split, StratifiedKFold
from pathlib import Path
from datetime import datetime
import json
from scipy.signal import butter, filtfilt, hilbert, detrend
from scipy.fft import fft
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½®å‚æ•° ====================
# æ•°æ®è·¯å¾„ï¼ˆå¤èµ›æ•°æ®é›†ï¼‰
TRAIN_ROOT = "/home/zxy_2024/FUSAI_Bearing_Fault_Diagnosis/å¤èµ›æ•°æ®é›†/å¤èµ›è®­ç»ƒé›†"  # è®­ç»ƒé›†æ–‡ä»¶å¤¹ï¼ˆ10ä¸ªå­æ–‡ä»¶å¤¹ï¼‰
TEST_ROOT = "/home/zxy_2024/FUSAI_Bearing_Fault_Diagnosis/å¤èµ›æ•°æ®é›†/å¤èµ›æµ‹è¯•é›†"    # æµ‹è¯•é›†æ–‡ä»¶å¤¹ï¼ˆæ‰€æœ‰æµ‹è¯•æ–‡ä»¶ï¼‰

# ä¼˜åŒ–å‚æ•°
N_TRIALS = 500  # Optuna è¯•éªŒæ¬¡æ•°
OPTIMIZATION_TIMEOUT = 2400000  # ä¼˜åŒ–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
N_FOLDS = 5  # 5æŠ˜äº¤å‰éªŒè¯

# æ¨¡å‹è®­ç»ƒå‚æ•°
TRAIN_EPOCHS = 100  # æ¯æ¬¡trialçš„è®­ç»ƒè½®æ•°ï¼ˆè¾ƒå°‘ä»¥åŠ å¿«ä¼˜åŒ–ï¼‰
BATCH_SIZE = 32
EARLY_STOPPING_PATIENCE = 60

# é‡‡æ ·é¢‘ç‡
FS = 20480
WINDOW_SIZE = 1024
STEP_SIZE = 1024

# æ ‡ç­¾æ˜ å°„ï¼ˆ10åˆ†ç±» - å¤èµ›æ•°æ®é›†ï¼‰
LABEL_MAP = {
    "inner_broken_train100": 0,
    "inner_missing_train125": 1,
    "inner_wear_train110": 2,
    "normal_train70": 3,
    "outer_broken_train120": 4,
    "outer_missing_train80": 5,
    "outer_wear_train140": 6,
    "roller_broken_train150": 7,
    "roller_missing_train90": 8,
    "roller_wear_train130": 9,
}
ID_TO_LABEL = {v: k.split("_train")[0] for k, v in LABEL_MAP.items()}

# ==================== ç‰¹å¾æå–å‡½æ•° ====================
def bandpass_filter(data, lowcut, highcut, fs, order=4):
    """å¸¦é€šæ»¤æ³¢"""
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    low = max(low, 1e-6)
    high = min(high, 0.999999)
    if low >= high:
        return data * 0  # è¿”å›é›¶ä¿¡å·
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, data)

def compute_fft_mag(vib, fs=FS):
    """è®¡ç®—FFTå¹…åº¦è°±"""
    N = len(vib)
    fft_vals = np.abs(fft(vib))[:N // 2]
    freqs = np.linspace(0, fs / 2, len(fft_vals))
    return freqs, fft_vals

def extract_band_psd(vib, spec_bands, out_len_spec, use_log=True):
    """æå–å¤šé¢‘æ®µé¢‘è°±ç‰¹å¾"""
    freqs, fft_vals = compute_fft_mag(vib)
    feats = []
    for (f1, f2) in spec_bands:
        mask = (freqs >= f1) & (freqs < f2)
        if not np.any(mask):
            feats.append(np.zeros(out_len_spec, dtype=np.float32))
            continue
        subf, subv = freqs[mask], (fft_vals[mask] ** 2)
        tgt = np.linspace(f1 if len(subf) > 0 else 0, f2, out_len_spec)
        if use_log:
            feats.append(np.log1p(np.interp(tgt, subf, subv)).astype(np.float32))
        else:
            feats.append(np.interp(tgt, subf, subv).astype(np.float32))
    return np.stack(feats, axis=0)

def extract_multi_env(vib, env_centers, env_bw, out_len_env, use_log=True):
    """æå–å¤šä¸­å¿ƒåŒ…ç»œè°±ç‰¹å¾"""
    vib_clean = detrend(vib)
    vib_clean = (vib_clean - vib_clean.mean()) / (vib_clean.std() + 1e-8)
    feats = []
    for center in env_centers:
        low = max(1, center - env_bw // 2)
        high = min(int(FS/2) - 1, center + env_bw // 2)
        try:
            filtered = bandpass_filter(vib_clean, low, high, FS)
            env = np.abs(hilbert(filtered))
            N = len(env)
            env_fft = np.abs(fft(env))[:N // 2]
            env_freqs = np.linspace(0, FS / 2, len(env_fft))
            mask = env_freqs <= 200
            if not np.any(mask):
                feats.append(np.zeros(out_len_env, dtype=np.float32))
                continue
            ef, ev = env_freqs[mask], (env_fft[mask] ** 2)
            tgt = np.linspace(0, 200, out_len_env)
            if use_log:
                feats.append(np.log1p(np.interp(tgt, ef, ev)).astype(np.float32))
            else:
                feats.append(np.interp(tgt, ef, ev).astype(np.float32))
        except Exception:
            feats.append(np.zeros(out_len_env, dtype=np.float32))
    return np.stack(feats, axis=0)

# ==================== æ•°æ®åŠ è½½å‡½æ•° ====================
def load_train_data_with_params(spec_bands, env_centers, env_bw, out_len_spec, out_len_env, use_log=True):
    """
    ä½¿ç”¨æŒ‡å®šå‚æ•°åŠ è½½å¹¶æå–è®­ç»ƒé›†ç‰¹å¾
    è¿”å›: (spec_features, env_features, labels)
    """
    spec_features = []
    env_features = []
    labels = []
    
    for folder_name, label in LABEL_MAP.items():
        folder_path = os.path.join(TRAIN_ROOT, folder_name)
        if not os.path.exists(folder_path):
            continue
        
        # å¯¹æ–‡ä»¶åˆ—è¡¨æ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
        file_list = sorted([f for f in os.listdir(folder_path) if f.endswith(".xlsx")])
        
        for fname in file_list:
            file_path = os.path.join(folder_path, fname)
            
            try:
                df = pd.read_excel(file_path, header=None, engine="openpyxl")
                vib_data = df.values[:, 1].astype(np.float32)  # åªå–ç¬¬äºŒåˆ—ï¼ˆæŒ¯åŠ¨ä¿¡å·ï¼‰
            except Exception:
                continue
            
            # åˆ‡å‰²çª—å£
            n_windows = (len(vib_data) - WINDOW_SIZE) // STEP_SIZE + 1
            if n_windows <= 0:
                continue
            
            for i in range(n_windows):
                start = i * STEP_SIZE
                end = start + WINDOW_SIZE
                window_vib = vib_data[start:end]
                
                # æå–ç‰¹å¾
                spec_feat = extract_band_psd(window_vib, spec_bands, out_len_spec, use_log)
                env_feat = extract_multi_env(window_vib, env_centers, env_bw, out_len_env, use_log)
                
                spec_features.append(spec_feat)
                env_features.append(env_feat)
                labels.append(label)
    
    return np.array(spec_features), np.array(env_features), np.array(labels, dtype=np.int64)

def load_test_data_with_params(spec_bands, env_centers, env_bw, out_len_spec, out_len_env, use_log=True):
    """
    ä½¿ç”¨æŒ‡å®šå‚æ•°åŠ è½½å¹¶æå–æµ‹è¯•é›†ç‰¹å¾
    è¿”å›: (spec_features, env_features, file_names)
    """
    spec_features = []
    env_features = []
    file_names = []
    
    if not os.path.exists(TEST_ROOT):
        return np.array([]), np.array([]), []
    
    # å¯¹æ–‡ä»¶åˆ—è¡¨æ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    file_list = sorted([f for f in os.listdir(TEST_ROOT) if f.endswith(".xlsx")])
    
    for fname in file_list:
        file_path = os.path.join(TEST_ROOT, fname)
        
        try:
            df = pd.read_excel(file_path, header=None, engine="openpyxl")
            vib_data = df.values[:, 1].astype(np.float32)
        except Exception:
            continue
        
        # åˆ‡å‰²çª—å£
        n_windows = (len(vib_data) - WINDOW_SIZE) // STEP_SIZE + 1
        if n_windows <= 0:
            continue
        
        for i in range(n_windows):
            start = i * STEP_SIZE
            end = start + WINDOW_SIZE
            window_vib = vib_data[start:end]
            
            spec_feat = extract_band_psd(window_vib, spec_bands, out_len_spec, use_log)
            env_feat = extract_multi_env(window_vib, env_centers, env_bw, out_len_env, use_log)
            
            spec_features.append(spec_feat)
            env_features.append(env_feat)
            file_names.append(fname)
    
    return np.array(spec_features), np.array(env_features), file_names

# ==================== ç®€åŒ–çš„Pæ¨¡å‹ ====================
class SimplePModel(nn.Module):
    """ç®€åŒ–çš„Pæ¨¡å‹ç”¨äºå¿«é€Ÿä¼˜åŒ–"""
    def __init__(self, n_spec_bands, n_env_centers, out_len, num_classes=10, dropout=0.3):
        super().__init__()
        
        # é¢‘è°±åˆ†æ”¯
        self.spec_branch = nn.Sequential(
            nn.Conv1d(n_spec_bands, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8),
            nn.Dropout(dropout)
        )
        
        # åŒ…ç»œåˆ†æ”¯
        self.env_branch = nn.Sequential(
            nn.Conv1d(n_env_centers, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8),
            nn.Dropout(dropout)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(32 * 8 * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, spec, env):
        # spec: (batch, n_spec_bands, out_len)
        # env: (batch, n_env_centers, out_len)
        spec_out = self.spec_branch(spec).view(spec.size(0), -1)
        env_out = self.env_branch(env).view(env.size(0), -1)
        combined = torch.cat([spec_out, env_out], dim=1)
        return self.classifier(combined)

# ==================== è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰====================
def train_and_evaluate_single_fold(spec_train, env_train, y_train, spec_val, env_val, y_val, device, n_spec_bands, n_env_centers, out_len, random_seed):
    """
    å•æŠ˜è®­ç»ƒå¹¶è¿”å›éªŒè¯é›†å‡†ç¡®ç‡
    """
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
    
    # å½’ä¸€åŒ–
    spec_mean = spec_train.mean(axis=(0, 2), keepdims=True)
    spec_std = spec_train.std(axis=(0, 2), keepdims=True) + 1e-8
    spec_train_norm = (spec_train - spec_mean) / spec_std
    spec_val_norm = (spec_val - spec_mean) / spec_std
    
    env_mean = env_train.mean(axis=(0, 2), keepdims=True)
    env_std = env_train.std(axis=(0, 2), keepdims=True) + 1e-8
    env_train_norm = (env_train - env_mean) / env_std
    env_val_norm = (env_val - env_mean) / env_std
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(
        torch.FloatTensor(spec_train_norm),
        torch.FloatTensor(env_train_norm),
        torch.LongTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(spec_val_norm),
        torch.FloatTensor(env_val_norm),
        torch.LongTensor(y_val)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆ10åˆ†ç±»ï¼‰
    model = SimplePModel(n_spec_bands, n_env_centers, out_len, num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒ
    best_val_acc = 0
    patience_counter = 0
    
    for epoch in range(TRAIN_EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        for spec_batch, env_batch, y_batch in train_loader:
            spec_batch = spec_batch.to(device)
            env_batch = env_batch.to(device)
            y_batch = y_batch.to(device)
            
            optimizer.zero_grad()
            outputs = model(spec_batch, env_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for spec_batch, env_batch, y_batch in val_loader:
                spec_batch = spec_batch.to(device)
                env_batch = env_batch.to(device)
                y_batch = y_batch.to(device)
                
                outputs = model(spec_batch, env_batch)
                _, predicted = torch.max(outputs.data, 1)
                val_total += y_batch.size(0)
                val_correct += (predicted == y_batch).sum().item()
        
        val_acc = 100 * val_correct / val_total
        
        # æ—©åœæ£€æŸ¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= EARLY_STOPPING_PATIENCE:
            break
    
    return best_val_acc

def train_and_evaluate_cross_validation(spec_features, env_features, labels, device, n_spec_bands, n_env_centers, out_len, n_folds=N_FOLDS, return_best_model=False, return_all_models=False):
    """
    5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒå¹¶è¿”å›å¹³å‡å‡†ç¡®ç‡
    å¦‚æœreturn_best_model=Trueï¼Œè¿”å›(mean_acc, std_acc, best_fold_acc, best_model_state)
    å¦‚æœreturn_all_models=Trueï¼Œè¿”å›(mean_acc, std_acc, best_fold_acc, all_fold_models, fold_accuracies)
    """
    # è®¾ç½®éšæœºç§å­
    random_seed = 42
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
    
    # åˆ›å»ºåˆ†å±‚KæŠ˜äº¤å‰éªŒè¯
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)
    
    fold_results = []
    fold_accuracies = []
    best_fold_acc = 0
    best_model_state = None
    all_fold_models = []  # å­˜å‚¨æ‰€æœ‰æŠ˜çš„æ¨¡å‹
    
    print(f"  å¼€å§‹{n_folds}æŠ˜äº¤å‰éªŒè¯...")
    
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(spec_features, labels)):
        print(f"    æŠ˜ {fold_idx + 1}/{n_folds}: è®­ç»ƒæ ·æœ¬ {len(train_idx)}, éªŒè¯æ ·æœ¬ {len(val_idx)}")
        
        # åˆ’åˆ†æ•°æ®
        spec_train_fold = spec_features[train_idx]
        env_train_fold = env_features[train_idx]
        y_train_fold = labels[train_idx]
        
        spec_val_fold = spec_features[val_idx]
        env_val_fold = env_features[val_idx]
        y_val_fold = labels[val_idx]
        
        # å¦‚æœåªéœ€è¦å‡†ç¡®ç‡ï¼Œä½¿ç”¨å¿«é€Ÿè¯„ä¼°
        if not return_best_model and not return_all_models:
            fold_acc = train_and_evaluate_single_fold(
                spec_train_fold, env_train_fold, y_train_fold,
                spec_val_fold, env_val_fold, y_val_fold,
                device, n_spec_bands, n_env_centers, out_len, 
                random_seed + fold_idx
            )
            fold_results.append(fold_acc)
            fold_accuracies.append(fold_acc)
        else:
            # éœ€è¦æ¨¡å‹æƒé‡ï¼Œè·å–å®Œæ•´æ¨¡å‹
            fold_model_state, fold_acc = train_and_get_best_model_with_acc(
                spec_train_fold, env_train_fold, y_train_fold,
                spec_val_fold, env_val_fold, y_val_fold,
                device, n_spec_bands, n_env_centers, out_len, 
                random_seed + fold_idx
            )
            fold_results.append(fold_acc)
            fold_accuracies.append(fold_acc)
            
            # ä¿å­˜è¿™ä¸€æŠ˜çš„æ¨¡å‹
            if return_all_models:
                all_fold_models.append(fold_model_state)
            
            # è®°å½•æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if return_best_model and fold_acc > best_fold_acc:
                best_fold_acc = fold_acc
                best_model_state = fold_model_state
        
        print(f"      æŠ˜ {fold_idx + 1} å‡†ç¡®ç‡: {fold_acc:.2f}%")
    
    mean_acc = np.mean(fold_results)
    std_acc = np.std(fold_results)
    best_acc = np.max(fold_results)
    
    print(f"  {n_folds}æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
    print(f"    å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in fold_results]}")
    print(f"    å¹³å‡å‡†ç¡®ç‡: {mean_acc:.2f}% (Â±{std_acc:.2f}%)")
    print(f"    æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    
    if return_all_models:
        return mean_acc, std_acc, best_acc, all_fold_models, fold_accuracies
    elif return_best_model:
        return mean_acc, std_acc, best_acc, best_model_state
    else:
        return mean_acc, std_acc, best_acc

def train_and_get_best_model_with_acc(spec_train, env_train, y_train, spec_val, env_val, y_val, device, n_spec_bands, n_env_centers, out_len, random_seed):
    """
    è®­ç»ƒå¹¶è¿”å›æœ€ä½³æ¨¡å‹çŠ¶æ€å’Œå‡†ç¡®ç‡
    è¿”å›: (best_model_state, best_val_acc)
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
    
    # å½’ä¸€åŒ–
    spec_mean = spec_train.mean(axis=(0, 2), keepdims=True)
    spec_std = spec_train.std(axis=(0, 2), keepdims=True) + 1e-8
    spec_train_norm = (spec_train - spec_mean) / spec_std
    spec_val_norm = (spec_val - spec_mean) / spec_std
    
    env_mean = env_train.mean(axis=(0, 2), keepdims=True)
    env_std = env_train.std(axis=(0, 2), keepdims=True) + 1e-8
    env_train_norm = (env_train - env_mean) / env_std
    env_val_norm = (env_val - env_mean) / env_std
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(
        torch.FloatTensor(spec_train_norm),
        torch.FloatTensor(env_train_norm),
        torch.LongTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(spec_val_norm),
        torch.FloatTensor(env_val_norm),
        torch.LongTensor(y_val)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆ10åˆ†ç±»ï¼‰
    model = SimplePModel(n_spec_bands, n_env_centers, out_len, num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒ
    best_val_acc = 0
    best_model_state = None
    patience_counter = 0
    
    for epoch in range(TRAIN_EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        for spec_batch, env_batch, y_batch in train_loader:
            spec_batch = spec_batch.to(device)
            env_batch = env_batch.to(device)
            y_batch = y_batch.to(device)
            
            optimizer.zero_grad()
            outputs = model(spec_batch, env_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for spec_batch, env_batch, y_batch in val_loader:
                spec_batch = spec_batch.to(device)
                env_batch = env_batch.to(device)
                y_batch = y_batch.to(device)
                
                outputs = model(spec_batch, env_batch)
                _, predicted = torch.max(outputs.data, 1)
                val_total += y_batch.size(0)
                val_correct += (predicted == y_batch).sum().item()
        
        val_acc = 100 * val_correct / val_total
        
        # æ—©åœæ£€æŸ¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= EARLY_STOPPING_PATIENCE:
            break
    
    return best_model_state, best_val_acc

def train_and_get_best_model(spec_train, env_train, y_train, spec_val, env_val, y_val, device, n_spec_bands, n_env_centers, out_len, random_seed):
    """
    è®­ç»ƒå¹¶è¿”å›æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    """
    model_state, _ = train_and_get_best_model_with_acc(
        spec_train, env_train, y_train, spec_val, env_val, y_val,
        device, n_spec_bands, n_env_centers, out_len, random_seed
    )
    return model_state


# ==================== Optunaä¼˜åŒ–ç›®æ ‡å‡½æ•° ====================
class BearingOptimizer:
    def __init__(self, device, save_dir):
        self.device = device
        self.save_dir = save_dir
        self.best_params = None
        self.best_score = 0
        self.all_trial_results = []  # ä¿å­˜æ‰€æœ‰è¯•éªŒçš„è¯¦ç»†ç»“æœ
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼šæœ€é«˜å¹³å‡å‡†ç¡®ç‡ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰
        self.best_mean_acc = 0  # å…¨å±€æœ€é«˜å¹³å‡å‡†ç¡®ç‡
        self.best_mean_model = None  # å¯¹åº”æ¨¡å‹
        self.best_mean_params = None  # å¯¹åº”å‚æ•°
        self.best_fold_results = []  # æœ€ä½³è¯•éªŒçš„å„æŠ˜ç»“æœ
        self.best_all_fold_models = []  # æœ€ä½³è¯•éªŒçš„æ‰€æœ‰5æŠ˜æ¨¡å‹
        self.best_fold_accuracies = []  # æœ€ä½³è¯•éªŒçš„å„æŠ˜å‡†ç¡®ç‡
    
    def objective(self, trial):
        """Optunaç›®æ ‡å‡½æ•°ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰"""
        try:
            # 1. å®šä¹‰æœç´¢ç©ºé—´
            n_spec_bands = trial.suggest_int('n_spec_bands', 3, 8)
            n_env_centers = trial.suggest_int('n_env_centers', 3, 8)
            
            # é¢‘è°±é¢‘æ®µå‚æ•°
            spec_low_freq = trial.suggest_int('spec_low_freq', 0, 2000)
            spec_high_freq = trial.suggest_int('spec_high_freq', 3000, 8000)
            
            # åŒ…ç»œå‚æ•°
            env_low_center = trial.suggest_int('env_low_center', 500, 2000)
            env_high_center = trial.suggest_int('env_high_center', 3000, 8000)
            env_bw = trial.suggest_int('env_bw', 200, 800)
            
            # ç‰¹å¾ç»´åº¦
            out_len_spec = trial.suggest_categorical('out_len_spec', [32, 48, 64, 96, 128, 256])
            out_len_env = trial.suggest_categorical('out_len_env', [32, 48, 64, 96, 128, 256])
            
            # å…¶ä»–å‚æ•°
            use_log = trial.suggest_categorical('use_log', [True, False])
            
            # ç¡®ä¿å‚æ•°æœ‰æ•ˆ
            if spec_low_freq >= spec_high_freq:
                return 0.0
            if env_low_center >= env_high_center:
                return 0.0
            
            # 2. ç”Ÿæˆé¢‘æ®µå’Œä¸­å¿ƒé¢‘ç‡
            spec_bands = list(zip(
                np.linspace(spec_low_freq, spec_high_freq, n_spec_bands + 1)[:-1],
                np.linspace(spec_low_freq, spec_high_freq, n_spec_bands + 1)[1:]
            ))
            
            env_centers = np.linspace(env_low_center, env_high_center, n_env_centers).tolist()
            
            print(f"\n{'='*60}")
            print(f"Trial {trial.number}: æå–ç‰¹å¾...")
            print(f"  é¢‘è°±é¢‘æ®µ: {n_spec_bands}æ®µ, {spec_low_freq}-{spec_high_freq}Hz")
            print(f"  åŒ…ç»œä¸­å¿ƒ: {n_env_centers}ä¸ª, {env_low_center}-{env_high_center}Hz, å¸¦å®½={env_bw}")
            print(f"  è¾“å‡ºç»´åº¦: spec={out_len_spec}, env={out_len_env}")
            
            # 3. æå–ç‰¹å¾
            spec_features, env_features, labels = load_train_data_with_params(
                spec_bands, env_centers, env_bw, out_len_spec, out_len_env, use_log
            )
            
            if len(labels) < 100:  # æ ·æœ¬æ•°å¤ªå°‘
                print(f"  æ ·æœ¬æ•°ä¸è¶³: {len(labels)}")
                return 0.0
            
            print(f"  ç‰¹å¾æå–å®Œæˆ: {spec_features.shape}, {env_features.shape}")
            
            # 4. 5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒå¹¶è¯„ä¼°ï¼ˆè·å–æ‰€æœ‰5æŠ˜çš„æ¨¡å‹ï¼‰
            print(f"  å¼€å§‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
            mean_acc, std_acc, best_acc, all_fold_models, fold_accuracies = train_and_evaluate_cross_validation(
                spec_features, env_features, labels,
                self.device, n_spec_bands, n_env_centers,
                max(out_len_spec, out_len_env), n_folds=N_FOLDS, return_all_models=True
            )
            
            print(f"  5æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
            print(f"    å¹³å‡å‡†ç¡®ç‡: {mean_acc:.2f}% (Â±{std_acc:.2f}%)")
            print(f"    æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
            print(f"    å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in fold_accuracies]}")
            print(f"{'='*60}")
            
            # 5. è®°å½•æœ€ä½³å‚æ•°ï¼ˆåŒ…å«ç‰¹å¾å‚æ•°é…ç½®ï¼‰
            trial_params = {
                'n_spec_bands': n_spec_bands,
                'n_env_centers': n_env_centers,
                'spec_bands': spec_bands,  # é¢‘è°±é¢‘æ®µé…ç½®
                'env_centers': env_centers,  # åŒ…ç»œä¸­å¿ƒé¢‘ç‡é…ç½®
                'env_bw': env_bw,  # åŒ…ç»œå¸¦å®½
                'out_len_spec': out_len_spec,
                'out_len_env': out_len_env,
                'use_log': use_log,
                'spec_low_freq': spec_low_freq,
                'spec_high_freq': spec_high_freq,
                'env_low_center': env_low_center,
                'env_high_center': env_high_center
            }
            
            trial_result = {
                'trial_number': trial.number,
                'params': trial_params.copy(),
                'mean_acc': mean_acc,
                'std_acc': std_acc,
                'best_acc': best_acc,
                'fold_accuracies': fold_accuracies,
                'n_folds': N_FOLDS
            }
            
            self.all_trial_results.append(trial_result)
            
            # æ›´æ–°å…¨å±€æœ€ä½³å¹³å‡å‡†ç¡®ç‡æ¨¡å‹
            if mean_acc > self.best_mean_acc:
                # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                old_model_path = f"{self.save_dir}/best_cv_model.pth"
                old_cv_models_dir = f"{self.save_dir}/best_cv_5fold_models"
                
                if os.path.exists(old_model_path):
                    try:
                        os.remove(old_model_path)
                        print(f"   å·²åˆ é™¤æ—§çš„æœ€ä½³æŠ˜æ¨¡å‹: {old_model_path}")
                    except Exception as e:
                        print(f"   è­¦å‘Š: åˆ é™¤æ—§æ¨¡å‹å¤±è´¥: {e}")
                
                if os.path.exists(old_cv_models_dir):
                    try:
                        shutil.rmtree(old_cv_models_dir)
                        print(f"   å·²åˆ é™¤æ—§çš„5æŠ˜æ¨¡å‹ç›®å½•: {old_cv_models_dir}")
                    except Exception as e:
                        print(f"   è­¦å‘Š: åˆ é™¤æ—§æ¨¡å‹ç›®å½•å¤±è´¥: {e}")
                
                self.best_mean_acc = mean_acc
                # ä¿å­˜æœ€ä½³æŠ˜çš„æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
                best_fold_idx = np.argmax(fold_accuracies)
                self.best_mean_model = all_fold_models[best_fold_idx]
                self.best_mean_params = trial_params.copy()
                self.best_mean_params['best_acc'] = best_acc
                self.best_mean_params['mean_acc'] = mean_acc
                self.best_mean_params['std_acc'] = std_acc
                self.best_fold_results = fold_accuracies.copy()
                self.best_all_fold_models = all_fold_models.copy()  # ä¿å­˜æ‰€æœ‰5æŠ˜æ¨¡å‹
                self.best_fold_accuracies = fold_accuracies.copy()
                
                # ç«‹å³ä¿å­˜æœ€ä½³å¹³å‡å‡†ç¡®ç‡æ¨¡å‹ï¼ˆæœ€ä½³æŠ˜ï¼‰
                model_path = f"{self.save_dir}/best_cv_model.pth"
                torch.save(self.best_mean_model, model_path)
                
                # ä¿å­˜æ‰€æœ‰5æŠ˜çš„æ¨¡å‹æƒé‡
                cv_models_dir = f"{self.save_dir}/best_cv_5fold_models"
                Path(cv_models_dir).mkdir(parents=True, exist_ok=True)
                
                for fold_idx, fold_model in enumerate(all_fold_models):
                    fold_model_path = f"{cv_models_dir}/fold_{fold_idx+1}_acc{fold_accuracies[fold_idx]:.2f}.pth"
                    torch.save(fold_model, fold_model_path)
                
                # ä¿å­˜ç‰¹å¾å‚æ•°é…ç½®ï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºè¯»å–ï¼‰
                feature_config = {
                    'trial_number': trial.number,
                    'mean_acc': mean_acc,
                    'std_acc': std_acc,
                    'best_acc': best_acc,
                    'fold_accuracies': fold_accuracies,
                    'feature_params': {
                        'spectrum': {
                            'n_bands': n_spec_bands,
                            'bands': [[float(f1), float(f2)] for f1, f2 in spec_bands],
                            'low_freq': float(spec_low_freq),
                            'high_freq': float(spec_high_freq),
                            'out_len': out_len_spec,
                            'use_log': use_log
                        },
                        'envelope': {
                            'n_centers': n_env_centers,
                            'centers': [float(c) for c in env_centers],
                            'bandwidth': float(env_bw),
                            'low_center': float(env_low_center),
                            'high_center': float(env_high_center),
                            'out_len': out_len_env,
                            'use_log': use_log
                        }
                    }
                }
                
                config_path = f"{cv_models_dir}/feature_config.json"
                with open(config_path, 'w') as f:
                    json.dump(feature_config, f, indent=2)
                
                print(f"\nğŸ¯ æ–°çš„æœ€ä½³å¹³å‡å‡†ç¡®ç‡ (è¯•éªŒ #{trial.number}): {mean_acc:.2f}% (Â±{std_acc:.2f}%)")
                print(f"   æœ€ä½³æŠ˜æ¨¡å‹å·²ä¿å­˜: {model_path}")
                print(f"   æ‰€æœ‰5æŠ˜æ¨¡å‹å·²ä¿å­˜åˆ°: {cv_models_dir}/")
                print(f"   ç‰¹å¾å‚æ•°é…ç½®å·²ä¿å­˜: {config_path}")
                print(f"   å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in fold_accuracies]}")
            
            # ä½¿ç”¨å¹³å‡å‡†ç¡®ç‡ä½œä¸ºä¼˜åŒ–ç›®æ ‡
            if mean_acc > self.best_score:
                self.best_score = mean_acc
                self.best_params = trial_params.copy()
                self.best_params['best_acc'] = best_acc
                self.best_params['mean_acc'] = mean_acc
                self.best_params['std_acc'] = std_acc
            
            # è¿”å›å¹³å‡å‡†ç¡®ç‡ä½œä¸ºä¼˜åŒ–ç›®æ ‡
            return mean_acc
            
        except Exception as e:
            print(f"Trial {trial.number} å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def optimize(self, n_trials=N_TRIALS):
        """æ‰§è¡Œä¼˜åŒ–"""
        print("ğŸš€ å¼€å§‹å‚æ•°ä¼˜åŒ–ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ç‰ˆ - 10åˆ†ç±»ï¼‰...")
        print(f"æ€»è¯•éªŒæ¬¡æ•°: {n_trials}")
        print(f"æ¯æ¬¡è¯•éªŒäº¤å‰éªŒè¯æŠ˜æ•°: {N_FOLDS}")
        print(f"æ¯æ¬¡è®­ç»ƒè½®æ•°: {TRAIN_EPOCHS}")
        print(f"ç»“æœä¿å­˜ç›®å½•: {self.save_dir}")
        
        study = optuna.create_study(
            direction='maximize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        study.optimize(self.objective, n_trials=n_trials, timeout=OPTIMIZATION_TIMEOUT)
        
        print(f"\nğŸ¯ ä¼˜åŒ–å®Œæˆ!")
        print(f"\nå…¨å±€æœ€ä½³ç»“æœ:")
        print(f"  æœ€ä½³å¹³å‡å‡†ç¡®ç‡: {self.best_mean_acc:.2f}%")
        print(f"  ä¼˜åŒ–ç›®æ ‡å‡†ç¡®ç‡: {self.best_score:.2f}%")
        
        print(f"\nğŸ“Š æœ€ä½³å¹³å‡å‡†ç¡®ç‡å¯¹åº”å‚æ•°:")
        for key, value in self.best_mean_params.items():
            if key not in ['best_acc', 'mean_acc', 'std_acc', 'spec_bands', 'env_centers']:
                print(f"  {key}: {value}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹çš„é…ç½®ä¿¡æ¯ï¼ˆåŒ…å«æ‰€æœ‰5æŠ˜æ¨¡å‹å’Œç‰¹å¾å‚æ•°ï¼‰
        cv_acc_info = {
            'model_type': 'best_cv_accuracy',
            'num_classes': 10,
            'mean_accuracy': self.best_mean_acc,
            'std_accuracy': self.best_mean_params.get('std_acc', 0),
            'best_fold_accuracy': self.best_mean_params.get('best_acc', 0),
            'fold_accuracies': self.best_fold_accuracies,
            'params': self.best_mean_params,
            'feature_params': {
                'spectrum': {
                    'n_bands': self.best_mean_params.get('n_spec_bands'),
                    'bands': [[float(f1), float(f2)] for f1, f2 in self.best_mean_params.get('spec_bands', [])],
                    'low_freq': float(self.best_mean_params.get('spec_low_freq', 0)),
                    'high_freq': float(self.best_mean_params.get('spec_high_freq', 0)),
                    'out_len': self.best_mean_params.get('out_len_spec'),
                    'use_log': self.best_mean_params.get('use_log', False)
                },
                'envelope': {
                    'n_centers': self.best_mean_params.get('n_env_centers'),
                    'centers': [float(c) for c in self.best_mean_params.get('env_centers', [])],
                    'bandwidth': float(self.best_mean_params.get('env_bw', 0)),
                    'low_center': float(self.best_mean_params.get('env_low_center', 0)),
                    'high_center': float(self.best_mean_params.get('env_high_center', 0)),
                    'out_len': self.best_mean_params.get('out_len_env'),
                    'use_log': self.best_mean_params.get('use_log', False)
                }
            },
            'description': 'å…¨å±€æœ€é«˜5æŠ˜äº¤å‰éªŒè¯å¹³å‡å‡†ç¡®ç‡æ¨¡å‹ï¼ˆ10åˆ†ç±»ï¼‰',
            'n_folds': N_FOLDS,
            'models_info': {
                'best_fold_model': 'best_cv_model.pth',
                'all_5fold_models_dir': 'best_cv_5fold_models/',
                'feature_config': 'best_cv_5fold_models/feature_config.json'
            }
        }
        
        with open(f"{self.save_dir}/best_cv_config.json", 'w') as f:
            json.dump(cv_acc_info, f, indent=2)
        
        print(f"\nğŸ’¾ æ¨¡å‹é…ç½®å·²ä¿å­˜:")
        print(f"  - best_cv_model.pth: æœ€ä½³æŠ˜æ¨¡å‹")
        print(f"  - best_cv_5fold_models/: æ‰€æœ‰5æŠ˜æ¨¡å‹ç›®å½•")
        print(f"    - fold_1_acc*.pth ~ fold_5_acc*.pth: 5ä¸ªæŠ˜çš„æ¨¡å‹æƒé‡")
        print(f"    - feature_config.json: ç‰¹å¾å‚æ•°é…ç½®")
        print(f"  - best_cv_config.json: å®Œæ•´é…ç½®ä¿¡æ¯")
        
        return self.best_params, self.best_score, study

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("è½´æ‰¿æ•…éšœè¯Šæ–­å‚æ•°ä¼˜åŒ–æµç¨‹ï¼ˆç¨³å®šç‰ˆ - å¤èµ›æ•°æ®é›†10åˆ†ç±»ï¼‰")
    print("="*60)
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    if not os.path.exists(TRAIN_ROOT):
        print(f"é”™è¯¯: è®­ç»ƒé›†è·¯å¾„ä¸å­˜åœ¨ {TRAIN_ROOT}")
        return None
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"/home/zxy_2024/FUSAI_Bearing_Fault_Diagnosis/results/optimization_cv_{timestamp}"
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    print(f"ç»“æœä¿å­˜ç›®å½•: {save_dir}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = BearingOptimizer(device, save_dir)
    
    # æ‰§è¡Œä¼˜åŒ–
    best_params, best_score, study = optimizer.optimize(n_trials=N_TRIALS)
    
    # ä¿å­˜æœ€ä½³å‚æ•°
    with open(f"{save_dir}/best_params.json", 'w') as f:
        json.dump(best_params, f, indent=2)
    
    # ä¿å­˜æ‰€æœ‰è¯•éªŒçš„è¯¦ç»†ç»“æœ
    with open(f"{save_dir}/all_trial_results.json", 'w') as f:
        json.dump(optimizer.all_trial_results, f, indent=2)
    
    # ä¿å­˜Optuna study
    with open(f"{save_dir}/study.pkl", 'wb') as f:
        pickle.dump(study, f)
    
    # ç”Ÿæˆè¯•éªŒç»“æœæ±‡æ€»
    summary = {
        'best_trial': {
            'mean_acc': best_params.get('mean_acc'),
            'std_acc': best_params.get('std_acc'),
            'best_acc': best_params.get('best_acc'),
            'params': {k: v for k, v in best_params.items() 
                      if k not in ['best_acc', 'mean_acc', 'std_acc']}
        },
        'optimization_config': {
            'n_trials': N_TRIALS,
            'n_folds': N_FOLDS,
            'train_epochs': TRAIN_EPOCHS,
            'early_stopping_patience': EARLY_STOPPING_PATIENCE,
            'num_classes': 10
        },
        'all_trials_summary': []
    }
    
    for result in optimizer.all_trial_results:
        summary['all_trials_summary'].append({
            'trial_number': result['trial_number'],
            'mean_acc': result['mean_acc'],
            'std_acc': result['std_acc'],
            'best_acc': result['best_acc'],
            'n_folds': result['n_folds']
        })
    
    with open(f"{save_dir}/optimization_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nğŸ’¾ ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"  æ¨¡å‹æƒé‡:")
    print(f"    - best_cv_model.pth: æœ€ä½³æŠ˜æ¨¡å‹")
    print(f"    - best_cv_5fold_models/: æ‰€æœ‰5æŠ˜æ¨¡å‹ç›®å½•")
    print(f"      * fold_1_acc*.pth ~ fold_5_acc*.pth: 5ä¸ªæŠ˜çš„æ¨¡å‹æƒé‡")
    print(f"      * feature_config.json: ç‰¹å¾å‚æ•°é…ç½®ï¼ˆé¢‘è°±å’ŒåŒ…ç»œè°±ï¼‰")
    print(f"  é…ç½®æ–‡ä»¶:")
    print(f"    - best_cv_config.json: æœ€ä½³5æŠ˜äº¤å‰éªŒè¯å®Œæ•´é…ç½®")
    print(f"    - best_params.json: æœ€ä½³å‚æ•°")
    print(f"  ç»“æœæ–‡ä»¶:")
    print(f"    - all_trial_results.json: æ‰€æœ‰è¯•éªŒè¯¦ç»†ç»“æœ")
    print(f"    - optimization_summary.json: ä¼˜åŒ–æ±‡æ€»")
    print(f"    - study.pkl: Optunaä¼˜åŒ–å†å²")
    
    # ========== ä½¿ç”¨æœ€ä½³5æŠ˜äº¤å‰éªŒè¯å‚æ•°ç”Ÿæˆæ•°æ®é›† ==========
    print(f"\n{'='*60}")
    print("ä½¿ç”¨æœ€ä½³5æŠ˜äº¤å‰éªŒè¯å‚æ•°ç”Ÿæˆæ•°æ®é›†...")
    print(f"{'='*60}")
    
    cv_params = optimizer.best_mean_params
    spec_bands_cv = [(f1, f2) for f1, f2 in cv_params['spec_bands']]
    env_centers_cv = cv_params['env_centers']
    env_bw_cv = cv_params['env_bw']
    out_len_spec_cv = cv_params['out_len_spec']
    out_len_env_cv = cv_params['out_len_env']
    use_log_cv = cv_params['use_log']
    
    # æå–è®­ç»ƒé›†ç‰¹å¾
    print("\næå–è®­ç»ƒé›†ç‰¹å¾ï¼ˆ5æŠ˜äº¤å‰éªŒè¯æœ€ä½³å‚æ•°ï¼‰...")
    spec_train_cv, env_train_cv, labels_train_cv = load_train_data_with_params(
        spec_bands_cv, env_centers_cv, env_bw_cv, out_len_spec_cv, out_len_env_cv, use_log_cv
    )
    
    print(f"è®­ç»ƒé›†ç‰¹å¾æå–å®Œæˆ:")
    print(f"  é¢‘è°±ç‰¹å¾: {spec_train_cv.shape}")
    print(f"  åŒ…ç»œç‰¹å¾: {env_train_cv.shape}")
    print(f"  æ ‡ç­¾: {labels_train_cv.shape}")
    print(f"  å„ç±»åˆ«æ ·æœ¬æ•°:")
    for label_id in range(10):  # 10åˆ†ç±»
        count = np.sum(labels_train_cv == label_id)
        print(f"    ç±»åˆ« {label_id} ({ID_TO_LABEL[label_id]}): {count} ä¸ªæ ·æœ¬")
    
    # æå–æµ‹è¯•é›†ç‰¹å¾
    print("\næå–æµ‹è¯•é›†ç‰¹å¾ï¼ˆ5æŠ˜äº¤å‰éªŒè¯æœ€ä½³å‚æ•°ï¼‰...")
    spec_test_cv, env_test_cv, test_files_cv = load_test_data_with_params(
        spec_bands_cv, env_centers_cv, env_bw_cv, out_len_spec_cv, out_len_env_cv, use_log_cv
    )
    
    if len(spec_test_cv) > 0:
        print(f"æµ‹è¯•é›†ç‰¹å¾æå–å®Œæˆ:")
        print(f"  é¢‘è°±ç‰¹å¾: {spec_test_cv.shape}")
        print(f"  åŒ…ç»œç‰¹å¾: {env_test_cv.shape}")
        print(f"  æ–‡ä»¶æ•°: {len(set(test_files_cv))}")
    else:
        print("æµ‹è¯•é›†è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ•°æ®")
    
    # ä¿å­˜5æŠ˜äº¤å‰éªŒè¯æ•°æ®é›†
    dataset_cv = {
        'x_train': {
            'spec': spec_train_cv,
            'env': env_train_cv
        },
        'y_train': labels_train_cv,
        'x_test': {
            'spec': spec_test_cv,
            'env': env_test_cv
        },
        'test_files': test_files_cv,
        'label_map': LABEL_MAP,
        'id_to_label': ID_TO_LABEL,
        'best_params': cv_params,
        'best_score': optimizer.best_mean_acc,
        'model_type': 'best_cv_accuracy',
        'metadata': {
            'fs': FS,
            'window_size': WINDOW_SIZE,
            'step_size': STEP_SIZE,
            'timestamp': timestamp,
            'n_train_samples': len(labels_train_cv),
            'n_test_samples': len(spec_test_cv) if len(spec_test_cv) > 0 else 0,
            'optimization_mode': 'cv',
            'n_folds': N_FOLDS,
            'num_classes': 10,
            'mean_acc': cv_params.get('mean_acc'),
            'std_acc': cv_params.get('std_acc'),
            'best_fold_acc': cv_params.get('best_acc'),
            'description': 'ä½¿ç”¨å…¨å±€æœ€ä½³5æŠ˜äº¤å‰éªŒè¯å¹³å‡å‡†ç¡®ç‡å‚æ•°ç”Ÿæˆçš„æ•°æ®é›†ï¼ˆ10åˆ†ç±»ï¼‰'
        }
    }
    
    dataset_cv_path = f"{save_dir}/dataset_best_cv.pkl"
    with open(dataset_cv_path, 'wb') as f:
        pickle.dump(dataset_cv, f)
    
    print(f"\nğŸ’¾ 5æŠ˜äº¤å‰éªŒè¯æ•°æ®é›†å·²ä¿å­˜: {dataset_cv_path}")
    
    # æ‰“å°æ•°æ®é›†ç»“æ„æ€»ç»“
    print(f"\n{'='*60}")
    print("æ•°æ®é›†ç”Ÿæˆå®Œæˆæ€»ç»“")
    print(f"{'='*60}")
    print(f"\n5æŠ˜äº¤å‰éªŒè¯æ•°æ®é›†: {dataset_cv_path}")
    print(f"   å¹³å‡å‡†ç¡®ç‡: {optimizer.best_mean_acc:.2f}% (Â±{cv_params.get('std_acc', 0):.2f}%)")
    print(f"   æœ€ä½³æŠ˜å‡†ç¡®ç‡: {cv_params.get('best_acc', 0):.2f}%")
    print(f"   è®­ç»ƒé›†: {spec_train_cv.shape[0]} æ ·æœ¬")
    if len(spec_test_cv) > 0:
        print(f"   æµ‹è¯•é›†: {spec_test_cv.shape[0]} æ ·æœ¬")
    
    print(f"\n{'='*60}")
    print("âœ… ä¼˜åŒ–æµç¨‹å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  æ¨¡å‹æƒé‡:")
    print(f"    1. best_cv_model.pth - æœ€ä½³æŠ˜æ¨¡å‹ ({optimizer.best_mean_acc:.2f}%)")
    print(f"    2. best_cv_5fold_models/ - æ‰€æœ‰5æŠ˜æ¨¡å‹ç›®å½•")
    print(f"       - fold_1_acc*.pth ~ fold_5_acc*.pth: 5ä¸ªæŠ˜çš„æ¨¡å‹æƒé‡")
    print(f"       - feature_config.json: ç‰¹å¾å‚æ•°é…ç½®ï¼ˆé¢‘è°±å’ŒåŒ…ç»œè°±å‚æ•°ï¼‰")
    print(f"  æ¨¡å‹é…ç½®:")
    print(f"    3. best_cv_config.json - æœ€ä½³5æŠ˜äº¤å‰éªŒè¯å®Œæ•´é…ç½®ï¼ˆåŒ…å«æ‰€æœ‰ç‰¹å¾å‚æ•°ï¼‰")
    print(f"  æ•°æ®é›†:")
    print(f"    4. dataset_best_cv.pkl - 5æŠ˜äº¤å‰éªŒè¯æœ€ä½³å‚æ•°æ•°æ®é›†")
    print(f"  ä¼˜åŒ–ç»“æœ:")
    print(f"    5. best_params.json - æœ€ä½³å‚æ•°")
    print(f"    6. all_trial_results.json - æ‰€æœ‰è¯•éªŒè¯¦ç»†ç»“æœ")
    print(f"    7. optimization_summary.json - ä¼˜åŒ–æ±‡æ€»")
    print(f"    8. study.pkl - Optunaä¼˜åŒ–å†å²")
    print(f"\næœ€ä½³ç»“æœ:")
    print(f"  å…¨å±€æœ€ä½³å¹³å‡å‡†ç¡®ç‡: {optimizer.best_mean_acc:.2f}%")
    print(f"  ä¼˜åŒ–ç›®æ ‡å‡†ç¡®ç‡: {best_score:.2f}%")
    print(f"  å‡†ç¡®ç‡æ ‡å‡†å·®: {best_params.get('std_acc', 0):.2f}%")
    print(f"  äº¤å‰éªŒè¯æŠ˜æ•°: {N_FOLDS}")
    print(f"  åˆ†ç±»ç±»åˆ«æ•°: 10")
    
    return best_params, best_score, save_dir, optimizer

if __name__ == "__main__":
    result = main()
    if result is not None:
        best_params, best_score, save_dir, optimizer = result
        print(f"\n{'='*60}")
        print("åç»­æ­¥éª¤:")
        print(f"{'='*60}")
        print(f"1. ä½¿ç”¨æœ€ä½³5æŠ˜äº¤å‰éªŒè¯æ¨¡å‹è¿›è¡Œé¢„æµ‹:")
        print(f"   æœ€ä½³æŠ˜æ¨¡å‹: {save_dir}/best_cv_model.pth")
        print(f"   æ‰€æœ‰5æŠ˜æ¨¡å‹: {save_dir}/best_cv_5fold_models/")
        print(f"      - fold_1_acc*.pth ~ fold_5_acc*.pth: 5ä¸ªæŠ˜çš„æ¨¡å‹æƒé‡")
        print(f"   ç‰¹å¾å‚æ•°é…ç½®: {save_dir}/best_cv_5fold_models/feature_config.json")
        print(f"   æ•°æ®é›†: {save_dir}/dataset_best_cv.pkl")
        print(f"   å¹³å‡å‡†ç¡®ç‡: {optimizer.best_mean_acc:.2f}% (Â±{optimizer.best_mean_params.get('std_acc', 0):.2f}%)")
        print(f"   æœ€ä½³æŠ˜å‡†ç¡®ç‡: {optimizer.best_mean_params.get('best_acc', 0):.2f}%")
        print(f"   å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in optimizer.best_fold_accuracies]}")
        print(f"")
        print(f"2. æŸ¥çœ‹ç‰¹å¾å‚æ•°é…ç½®:")
        print(f"   é¢‘è°±å‚æ•°: {save_dir}/best_cv_5fold_models/feature_config.json")
        print(f"   åŒ…å«: é¢‘æ®µæ•°é‡ã€é¢‘æ®µèŒƒå›´ã€è¾“å‡ºç»´åº¦ã€æ˜¯å¦ä½¿ç”¨å¯¹æ•°ç­‰")
        print(f"   åŒ…ç»œå‚æ•°: ä¸­å¿ƒé¢‘ç‡ã€å¸¦å®½ã€è¾“å‡ºç»´åº¦ç­‰")
        print(f"")
        print(f"3. æŸ¥çœ‹è¯¦ç»†çš„è¯•éªŒç»“æœ:")
        print(f"   æ‰€æœ‰è¯•éªŒç»“æœ: {save_dir}/all_trial_results.json")
        print(f"   ä¼˜åŒ–æ±‡æ€»: {save_dir}/optimization_summary.json")
        print(f"{'='*60}")

