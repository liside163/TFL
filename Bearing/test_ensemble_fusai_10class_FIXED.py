#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤èµ›10åˆ†ç±»é›†æˆé¢„æµ‹æµ‹è¯•è„šæœ¬ï¼ˆTTAå¢å¼ºç‰ˆï¼‰- ä¿®å¤å½’ä¸€åŒ–é—®é¢˜

ğŸ”§ å…³é”®ä¿®å¤:
1. æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„mean/stdè¿›è¡Œå½’ä¸€åŒ–
2. ä¿å­˜å’ŒåŠ è½½å½’ä¸€åŒ–å‚æ•°
3. ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§
"""

import os
import sys
import argparse
import pickle
import json
import re
from collections import Counter
from tqdm import tqdm

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# ==================== é…ç½®å‚æ•° ====================
DATASET_PATH = "/home/zxy_2024/FUSAI_Bearing_Fault_Diagnosis/results/optimization_cv_20251105_105552/trial_54/dataset_trial_54.pkl"
MODEL_DIR = "/home/zxy_2024/FUSAI_Bearing_Fault_Diagnosis/results/optimization_cv_20251105_105552/trial_54"
TTA_PARAMS_PATH = "/home/zxy_2024/FUSAI_Bearing_Fault_Diagnosis/tta_best_params.json"
OUTPUT_DIR = f"{MODEL_DIR}/test_predictions_FIXED"

# ==================== SimplePModel æ¨¡å‹å®šä¹‰ ====================
class SimplePModel(nn.Module):
    """ç®€åŒ–çš„Pæ¨¡å‹ï¼ˆå¤èµ›10åˆ†ç±»ç‰ˆæœ¬ï¼‰"""
    def __init__(self, n_spec_bands, n_env_centers, out_len, num_classes=10, dropout=0.3):
        super().__init__()
        
        self.spec_branch = nn.Sequential(
            nn.Conv1d(n_spec_bands, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8),
            nn.Dropout(dropout)
        )
        
        self.env_branch = nn.Sequential(
            nn.Conv1d(n_env_centers, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8),
            nn.Dropout(dropout)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(32 * 8 * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, spec, env):
        spec_out = self.spec_branch(spec).view(spec.size(0), -1)
        env_out = self.env_branch(env).view(env.size(0), -1)
        combined = torch.cat([spec_out, env_out], dim=1)
        return self.classifier(combined)

# ==================== å½’ä¸€åŒ–å‡½æ•° ====================
def compute_normalization_stats(spec_train, env_train):
    """
    è®¡ç®—è®­ç»ƒé›†çš„å½’ä¸€åŒ–ç»Ÿè®¡é‡
    è¿”å›: (spec_mean, spec_std, env_mean, env_std)
    """
    print("  è®¡ç®—è®­ç»ƒé›†å½’ä¸€åŒ–ç»Ÿè®¡é‡...")
    
    # æŒ‰é€šé“è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    spec_mean = spec_train.mean(axis=(0, 2), keepdims=True)  # (1, n_bands, 1)
    spec_std = spec_train.std(axis=(0, 2), keepdims=True) + 1e-8
    
    env_mean = env_train.mean(axis=(0, 2), keepdims=True)  # (1, n_centers, 1)
    env_std = env_train.std(axis=(0, 2), keepdims=True) + 1e-8
    
    print(f"    é¢‘è°±: mean={spec_mean.mean():.4f}, std={spec_std.mean():.4f}")
    print(f"    åŒ…ç»œ: mean={env_mean.mean():.4f}, std={env_std.mean():.4f}")
    
    return spec_mean, spec_std, env_mean, env_std

def normalize_data(spec, env, spec_mean, spec_std, env_mean, env_std):
    """
    ä½¿ç”¨ç»™å®šçš„ç»Ÿè®¡é‡å½’ä¸€åŒ–æ•°æ®
    """
    spec_norm = (spec - spec_mean) / spec_std
    env_norm = (env - env_mean) / env_std
    return spec_norm, env_norm

# ==================== TTAå¢å¼ºå‡½æ•° ====================
def _add_noise(arr, snr_db):
    """æ·»åŠ é«˜æ–¯å™ªå£°"""
    sig_pow = np.mean(arr ** 2)
    if sig_pow < 1e-10:
        return arr
    noise_pow = sig_pow / (10 ** (snr_db / 10))
    noise = np.random.normal(0, np.sqrt(noise_pow), size=arr.shape)
    return arr + noise

def _time_shift(arr, shift_ratio):
    """æ—¶é—´å¹³ç§»"""
    L = arr.shape[-1]
    shift = int(np.round(L * shift_ratio))
    return np.roll(arr, shift, axis=-1)

def _scale(arr, factor):
    """å¹…å€¼ç¼©æ”¾"""
    return arr * factor

def _mirror(arr):
    """é•œåƒç¿»è½¬"""
    return np.flip(arr, axis=-1).copy()

def augment_sample_tta(spec_sample, env_sample, tta_params):
    """ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆ8ä¸ªTTAå˜ä½“"""
    shift_ratio = tta_params['shift_ratio']
    snr_db = tta_params['snr_db']
    scale_low = tta_params['scale_low']
    scale_high = tta_params['scale_high']
    
    spec_vars = []
    env_vars = []

    # 1-8: å„ç§å¢å¼º
    spec_vars.append(spec_sample.copy())
    env_vars.append(env_sample.copy())
    
    spec_vars.append(_time_shift(spec_sample, +shift_ratio))
    env_vars.append(_time_shift(env_sample, +shift_ratio))
    
    spec_vars.append(_time_shift(spec_sample, -shift_ratio))
    env_vars.append(_time_shift(env_sample, -shift_ratio))
    
    spec_vars.append(_add_noise(spec_sample, snr_db))
    env_vars.append(_add_noise(env_sample, snr_db))
    
    spec_vars.append(_add_noise(spec_sample, snr_db))
    env_vars.append(_add_noise(env_sample, snr_db))
    
    spec_vars.append(_scale(spec_sample, scale_low))
    env_vars.append(_scale(env_sample, scale_low))
    
    spec_vars.append(_scale(spec_sample, scale_high))
    env_vars.append(_scale(env_sample, scale_high))
    
    spec_vars.append(_mirror(spec_sample))
    env_vars.append(_mirror(env_sample))

    return spec_vars, env_vars

# ==================== TTAé›†æˆé¢„æµ‹ ====================
def tta_ensemble_predict(models, spec_test_norm, env_test_norm, device, tta_params, T=1.0, fusion='softmax'):
    """
    ä½¿ç”¨TTA + ensembleåœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
    æ³¨æ„: spec_test_norm å’Œ env_test_norm å¿…é¡»å·²ç»å½’ä¸€åŒ–
    """
    n_samples = spec_test_norm.shape[0]
    final_preds = []
    decision_details = []

    for idx in tqdm(range(n_samples), desc="TTAé›†æˆé¢„æµ‹"):
        spec_s = spec_test_norm[idx]
        env_s = env_test_norm[idx]

        # ç”Ÿæˆ8ä¸ªTTAå˜ä½“
        spec_vars, env_vars = augment_sample_tta(spec_s, env_s, tta_params)

        all_logits = []
        all_preds = []

        with torch.no_grad():
            for model in models:
                for spec_v, env_v in zip(spec_vars, env_vars):
                    spec_t = torch.tensor(spec_v, dtype=torch.float32).unsqueeze(0).to(device)
                    env_t = torch.tensor(env_v, dtype=torch.float32).unsqueeze(0).to(device)
                    
                    logits = model(spec_t, env_t)
                    all_logits.append(logits.cpu().numpy()[0])
                    pred = torch.argmax(logits, dim=1).item()
                    all_preds.append(pred)

        all_logits = np.array(all_logits)
        
        if fusion == 'softmax':
            scaled_logits = all_logits / T
            probs = np.exp(scaled_logits - np.max(scaled_logits, axis=1, keepdims=True))
            probs = probs / np.sum(probs, axis=1, keepdims=True)
            avg_probs = np.mean(probs, axis=0)
            final_pred = np.argmax(avg_probs)
            confidence = avg_probs[final_pred]
        else:  # vote
            vote_counts = Counter(all_preds)
            final_pred = vote_counts.most_common(1)[0][0]
            confidence = vote_counts[final_pred] / len(all_preds)
            avg_probs = np.zeros(10)
            for pred_id, count in vote_counts.items():
                avg_probs[pred_id] = count / len(all_preds)

        final_preds.append(final_pred)
        
        detail = {
            'individual_predictions': all_preds,
            'vote_counts': dict(Counter(all_preds)),
            'avg_probabilities': avg_probs.tolist(),
            'confidence': float(confidence)
        }
        decision_details.append(detail)

    return np.array(final_preds), decision_details

# ==================== ä¸»å‡½æ•° ====================
def main():
    parser = argparse.ArgumentParser(description='å¤èµ›10åˆ†ç±»é›†æˆé¢„æµ‹ï¼ˆä¿®å¤å½’ä¸€åŒ–ï¼‰')
    parser.add_argument('--dataset', type=str, default=DATASET_PATH)
    parser.add_argument('--model_dir', type=str, default=MODEL_DIR)
    parser.add_argument('--tta_params', type=str, default=TTA_PARAMS_PATH)
    parser.add_argument('--output', type=str, default=OUTPUT_DIR)
    parser.add_argument('--vote', action='store_true')
    parser.add_argument('--no_tta', action='store_true')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ•°æ®é›†
    print(f"\nåŠ è½½æ•°æ®é›†: {args.dataset}")
    with open(args.dataset, 'rb') as f:
        dataset = pickle.load(f)
    
    spec_train = dataset['x_train']['spec']
    env_train = dataset['x_train']['env']
    spec_test = dataset['x_test']['spec']
    env_test = dataset['x_test']['env']
    test_files = dataset['test_files']
    label_map = dataset['label_map']
    id_to_label = dataset['id_to_label']
    
    test_sample_names = sorted(list(set(test_files)))
    
    print(f"  è®­ç»ƒé›†: spec {spec_train.shape}, env {env_train.shape}")
    print(f"  æµ‹è¯•é›†: spec {spec_test.shape}, env {env_test.shape}")
    print(f"  æµ‹è¯•æ–‡ä»¶æ•°: {len(test_sample_names)}")
    
    # ğŸ”§ æ£€æŸ¥æ•°æ®æ˜¯å¦å·²å½’ä¸€åŒ–
    is_normalized = dataset.get('metadata', {}).get('normalized', False)
    
    if is_normalized:
        print(f"\nâœ“ æ•°æ®å·²å½’ä¸€åŒ–ï¼ˆpklæ–‡ä»¶ä¸­ï¼‰- ç›´æ¥ä½¿ç”¨")
        print(f"  è®­ç»ƒé›†ç»Ÿè®¡: spec mean={spec_train.mean():.4f}, std={spec_train.std():.4f}")
        print(f"  æµ‹è¯•é›†ç»Ÿè®¡: spec mean={spec_test.mean():.4f}, std={spec_test.std():.4f}")
        
        # ç›´æ¥ä½¿ç”¨å·²å½’ä¸€åŒ–çš„æ•°æ®
        spec_train_norm = spec_train
        env_train_norm = env_train
        spec_test_norm = spec_test
        env_test_norm = env_test
        
        # ä»datasetä¸­è·å–å½’ä¸€åŒ–å‚æ•°
        if 'normalization' in dataset:
            norm_params = {
                'spec_mean': dataset['normalization']['spec_mean'].tolist() if hasattr(dataset['normalization']['spec_mean'], 'tolist') else dataset['normalization']['spec_mean'],
                'spec_std': dataset['normalization']['spec_std'].tolist() if hasattr(dataset['normalization']['spec_std'], 'tolist') else dataset['normalization']['spec_std'],
                'env_mean': dataset['normalization']['env_mean'].tolist() if hasattr(dataset['normalization']['env_mean'], 'tolist') else dataset['normalization']['env_mean'],
                'env_std': dataset['normalization']['env_std'].tolist() if hasattr(dataset['normalization']['env_std'], 'tolist') else dataset['normalization']['env_std']
            }
        else:
            norm_params = None
    else:
        print(f"\nâš ï¸  æ•°æ®æœªå½’ä¸€åŒ– - éœ€è¦æ‰‹åŠ¨å½’ä¸€åŒ–")
        print(f"  åŸå§‹è®­ç»ƒé›†ç»Ÿè®¡: spec mean={spec_train.mean():.4f}, std={spec_train.std():.4f}")
        
        # è®¡ç®—è®­ç»ƒé›†çš„å½’ä¸€åŒ–ç»Ÿè®¡é‡
        spec_mean, spec_std, env_mean, env_std = compute_normalization_stats(spec_train, env_train)
        
        # å½’ä¸€åŒ–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        spec_train_norm, env_train_norm = normalize_data(spec_train, env_train, spec_mean, spec_std, env_mean, env_std)
        spec_test_norm, env_test_norm = normalize_data(spec_test, env_test, spec_mean, spec_std, env_mean, env_std)
        
        print(f"  å½’ä¸€åŒ–å:")
        print(f"    è®­ç»ƒé›†: spec mean={spec_train_norm.mean():.4f}, std={spec_train_norm.std():.4f}")
        print(f"    æµ‹è¯•é›†: spec mean={spec_test_norm.mean():.4f}, std={spec_test_norm.std():.4f}")
        
        # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
        norm_params = {
            'spec_mean': spec_mean.tolist(),
            'spec_std': spec_std.tolist(),
            'env_mean': env_mean.tolist(),
            'env_std': env_std.tolist()
        }
    
    # ä¿å­˜å½’ä¸€åŒ–å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    if norm_params:
        os.makedirs(args.output, exist_ok=True)
        with open(f"{args.output}/normalization_params.json", 'w') as f:
            json.dump(norm_params, f, indent=2)
        print(f"  âœ“ å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜: {args.output}/normalization_params.json")
    
    # 2. åŠ è½½TTAå‚æ•°
    print(f"\nåŠ è½½TTAå‚æ•°: {args.tta_params}")
    with open(args.tta_params, 'r') as f:
        tta_params = json.load(f)
    
    if args.no_tta:
        tta_params = {'shift_ratio': 0.0, 'snr_db': 1000.0, 'scale_low': 1.0, 'scale_high': 1.0}
    
    # 3. åŠ è½½æ¨¡å‹
    print(f"\nåŠ è½½æ¨¡å‹: {args.model_dir}")
    model_files = sorted([f for f in os.listdir(args.model_dir) if f.startswith('fold_') and f.endswith('.pth')])
    
    feature_params = dataset.get('feature_params', dataset.get('config', {}).get('feature_params', {}))
    n_spec_bands = feature_params['spectrum']['n_bands']
    n_env_centers = feature_params['envelope']['n_centers']
    out_len = max(feature_params['spectrum']['out_len'], feature_params['envelope']['out_len'])
    
    models = []
    for model_file in model_files:
        model_path = os.path.join(args.model_dir, model_file)
        model = SimplePModel(n_spec_bands, n_env_centers, out_len, num_classes=10, dropout=0.3).to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        models.append(model)
    
    print(f"  æˆåŠŸåŠ è½½ {len(models)} ä¸ªæ¨¡å‹")
    
    # 4. TTAé›†æˆé¢„æµ‹ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„æ•°æ®ï¼‰
    print(f"\nå¼€å§‹é¢„æµ‹...")
    
    file_to_indices = {}
    for idx, filename in enumerate(test_files):
        if filename not in file_to_indices:
            file_to_indices[filename] = []
        file_to_indices[filename].append(idx)
    
    # ğŸ”§ ä½¿ç”¨å½’ä¸€åŒ–åçš„æµ‹è¯•æ•°æ®
    window_predictions, decision_details = tta_ensemble_predict(
        models, spec_test_norm, env_test_norm, device, tta_params, T=1.0, fusion='softmax'
    )
    
    # æŒ‰æ–‡ä»¶èšåˆ
    file_predictions = {}
    file_confidences = {}  # å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„ç½®ä¿¡åº¦
    
    for filename in test_sample_names:
        indices = file_to_indices[filename]
        window_preds = window_predictions[indices]
        
        # å¯¹è¯¥æ–‡ä»¶çš„æ‰€æœ‰çª—å£è¿›è¡ŒæŠ•ç¥¨
        vote_counts = Counter(window_preds)
        final_pred = vote_counts.most_common(1)[0][0]
        file_predictions[filename] = final_pred
        
        # è®¡ç®—è¯¥æ–‡ä»¶çš„å¹³å‡ç½®ä¿¡åº¦ï¼ˆæ‰€æœ‰çª—å£çš„å¹³å‡ï¼‰
        window_confidences = [decision_details[idx]['confidence'] for idx in indices]
        avg_confidence = np.mean(window_confidences)
        file_confidences[filename] = avg_confidence
    
    # 5. ç”Ÿæˆç»“æœ
    print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")
    
    simple_results = ["æµ‹è¯•é›†åç§°\tæ•…éšœç±»å‹"]
    detailed_results = ["æµ‹è¯•é›†åç§°\tæ•…éšœç±»å‹\tå¹³å‡ç½®ä¿¡åº¦\tçª—å£æ•°"]
    
    for filename in test_sample_names:
        pred_id = file_predictions[filename]
        pred_label = id_to_label[int(pred_id)]
        clean_name = filename.replace('.xlsx', '').replace('.XLSX', '')
        confidence = file_confidences[filename]
        n_windows = len(file_to_indices[filename])
        
        simple_results.append(f"{clean_name}\t{pred_label}")
        detailed_results.append(f"{clean_name}\t{pred_label}\t{confidence:.4f}\t{n_windows}")
    
    # 6. ä¿å­˜ç»“æœ
    output_txt = os.path.join(args.output, "test_predictions.txt")
    output_detailed = os.path.join(args.output, "test_predictions_detailed.txt")
    
    with open(output_txt, 'w', encoding='utf-8-sig') as f:
        f.write('\n'.join(simple_results))
    
    with open(output_detailed, 'w', encoding='utf-8-sig') as f:
        f.write('\n'.join(detailed_results))
    
    # 7. è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
    all_confidences = list(file_confidences.values())
    avg_confidence = np.mean(all_confidences)
    min_confidence = np.min(all_confidences)
    max_confidence = np.max(all_confidences)
    std_confidence = np.std(all_confidences)
    median_confidence = np.median(all_confidences)
    
    print(f"\nâœ… é¢„æµ‹å®Œæˆ!")
    print(f"  æµ‹è¯•æ–‡ä»¶æ•°: {len(test_sample_names)}")
    print(f"  ç®€å•ç»“æœ: {output_txt}")
    print(f"  è¯¦ç»†ç»“æœ: {output_detailed}")
    
    # 8. ç½®ä¿¡åº¦ç»Ÿè®¡
    print(f"\nğŸ“Š ç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f} ({avg_confidence*100:.2f}%)")
    print(f"  ä¸­ä½æ•°ç½®ä¿¡åº¦: {median_confidence:.4f} ({median_confidence*100:.2f}%)")
    print(f"  æœ€å°ç½®ä¿¡åº¦: {min_confidence:.4f} ({min_confidence*100:.2f}%)")
    print(f"  æœ€å¤§ç½®ä¿¡åº¦: {max_confidence:.4f} ({max_confidence*100:.2f}%)")
    print(f"  ç½®ä¿¡åº¦æ ‡å‡†å·®: {std_confidence:.4f}")
    
    # ç½®ä¿¡åº¦åˆ†æ®µç»Ÿè®¡
    high_conf = sum(1 for c in all_confidences if c >= 0.8)
    med_conf = sum(1 for c in all_confidences if 0.5 <= c < 0.8)
    low_conf = sum(1 for c in all_confidences if c < 0.5)
    
    print(f"\n  ç½®ä¿¡åº¦åˆ†æ®µ:")
    print(f"    é«˜ç½®ä¿¡åº¦ (>=80%): {high_conf} ({100.0*high_conf/len(all_confidences):.1f}%)")
    print(f"    ä¸­ç½®ä¿¡åº¦ (50-80%): {med_conf} ({100.0*med_conf/len(all_confidences):.1f}%)")
    print(f"    ä½ç½®ä¿¡åº¦ (<50%): {low_conf} ({100.0*low_conf/len(all_confidences):.1f}%)")
    
    # 9. é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡
    pred_counts = Counter(file_predictions.values())
    print("\nğŸ“Š é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ:")
    for label_id in sorted(pred_counts.keys()):
        label_name = id_to_label[label_id]
        count = pred_counts[label_id]
        percentage = 100.0 * count / len(test_sample_names)
        
        # è®¡ç®—è¯¥ç±»åˆ«çš„å¹³å‡ç½®ä¿¡åº¦
        class_confidences = [file_confidences[f] for f in test_sample_names if file_predictions[f] == label_id]
        class_avg_conf = np.mean(class_confidences) if class_confidences else 0
        
        print(f"  {label_name}: {count} ({percentage:.1f}%) - å¹³å‡ç½®ä¿¡åº¦: {class_avg_conf:.3f}")
    
    print("\nğŸ”§ ä¿®å¤è¯´æ˜:")
    print("  âœ“ æµ‹è¯•é›†å·²ä½¿ç”¨è®­ç»ƒé›†çš„mean/stdè¿›è¡Œå½’ä¸€åŒ–")
    print("  âœ“ ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•çš„ä¸€è‡´æ€§")
    print("  âœ“ å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜ä¾›åç»­ä½¿ç”¨")
    
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆï¼ˆå·²ä¿®å¤å½’ä¸€åŒ–é—®é¢˜ï¼‰!")
    print("="*60)

if __name__ == "__main__":
    main()

